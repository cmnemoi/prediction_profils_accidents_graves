{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# !pip install catboost (catboost-1.2.7)\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, roc_auc_score, mean_squared_error, confusion_matrix, r2_score\n",
    ")\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.models import Sequential\n",
    "# from tensorflow.layers import Dense, Dropout\n",
    "# from tensorflow.optimizers import Adam\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet('../data/dataset_features.parquet')\n",
    "target = 'total_cout_accident'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset.drop(target, axis=1), \n",
    "                                                    dataset[target], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_performance(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
    "    print(f'R-squared (R²): {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10.8252\n",
      "Root Mean Squared Error (RMSE): 3.2902\n",
      "R-squared (R²): 0.1751\n"
     ]
    }
   ],
   "source": [
    "mod_hist = HistGradientBoostingRegressor()\n",
    "mod_hist.fit(X_train, y_train)\n",
    "\n",
    "output_performance(mod_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Documents\\anaconda3\\envs\\cyu\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [17:38:38] WARNING: C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"verbose\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10.8541\n",
      "Root Mean Squared Error (RMSE): 3.2946\n",
      "R-squared (R²): 0.1729\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBRegressor(verbose=2) \n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "output_performance(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 10.8344\n",
      "Root Mean Squared Error (RMSE): 3.2916\n",
      "R-squared (R²): 0.1744\n"
     ]
    }
   ],
   "source": [
    "cat = CatBoostRegressor(verbose=0)\n",
    "cat.fit(X_train, y_train)\n",
    "\n",
    "output_performance(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Séparer les colonnes numériques et non numériques\n",
    "numerical_columns, non_numerical_columns = split_columns(dataset)\n",
    " \n",
    "# Exclure la colonne cible des colonnes d'entrée\n",
    "numerical_columns = [col for col in numerical_columns if col != 'gravite_blessure']\n",
    " \n",
    "# Définir les variables d'entrée et la cible\n",
    "X = dataset.drop(columns=['gravite_blessure'])\n",
    "y = dataset['gravite_blessure']\n",
    " \n",
    "# Diviser les ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "# Vérifier les colonnes disponibles dans X_train\n",
    "available_columns = X_train.columns.tolist()\n",
    " \n",
    "# Filtrer les colonnes disponibles\n",
    "numerical_columns = [col for col in numerical_columns if col in available_columns]\n",
    "non_numerical_columns = [col for col in non_numerical_columns if col in available_columns]\n",
    " \n",
    "# Vérifier les colonnes\n",
    "print(\"Colonnes numériques  :\", numerical_columns)\n",
    "print(\"Colonnes non numériques  :\", non_numerical_columns)\n",
    " \n",
    "# Pipeline de prétraitement\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    " \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, non_numerical_columns)\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Transformation des données\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    " \n",
    "# Modèles de classification supervisée utiliser\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500, random_state=42, verbose=2),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    #\"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, verbose=2),\n",
    "    \"Support Vector Machine\": SVC(kernel='linear', probability=True, random_state=42,verbose=2),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    #\"XGBoost\": XGBClassifier(objective='multi:softmax', num_class=5, random_state=42)\n",
    "}\n",
    " \n",
    "# Initialisation pour stocker les résultats\n",
    "results = {}\n",
    " \n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Modèle : {model_name} ---\")\n",
    "   \n",
    "    # Entraînement\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "   \n",
    "    # Prédictions\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    y_pred_prob = model.predict_proba(X_test_transformed) if hasattr(model, \"predict_proba\") else None\n",
    "   \n",
    "    # Métriques d'évaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "   \n",
    "    # AUC\n",
    "    if y_pred_prob is not None:\n",
    "        y_test_onehot = pd.get_dummies(y_test).values  # Encodage one-hot pour le calcul de l'AUC\n",
    "        auc = roc_auc_score(y_test_onehot, y_pred_prob, multi_class='ovr', average='weighted')\n",
    "    else:\n",
    "        auc = None\n",
    "   \n",
    "    # RMSE\n",
    "    if y_pred_prob is not None:\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_onehot, y_pred_prob))\n",
    "    else:\n",
    "        rmse = None  \n",
    "   \n",
    "    # Rapport de classification\n",
    "    print(\"Rapport de classification :\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "   \n",
    "    # Résultats\n",
    "    results[model_name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC\": auc,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Confusion Matrix\": cm\n",
    "    }\n",
    " \n",
    "# Réseau de neurones pour la classification multi-classes\n",
    "y_train_encoded = pd.get_dummies(y_train).values  # Encodage OneHot pour y_train\n",
    "y_test_encoded = pd.get_dummies(y_test).values    # Encodage OneHot pour y_test\n",
    " \n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_transformed.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(y.unique()), activation='softmax')  # Nombre de classes = len(y.unique())\n",
    "])\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    " \n",
    "print(\"\\n--- Réseau de Neurones ---\")\n",
    "nn_model.fit(X_train_transformed, y_train_encoded, epochs=20, batch_size=32, verbose=1)\n",
    " \n",
    "# Évaluation du réseau de neurones\n",
    "nn_loss, nn_accuracy = nn_model.evaluate(X_test_transformed, y_test_encoded, verbose=0)\n",
    "y_pred_prob_nn = nn_model.predict(X_test_transformed)\n",
    "nn_auc = roc_auc_score(y_test_encoded, y_pred_prob_nn, multi_class='ovr', average='weighted')\n",
    "nn_rmse = np.sqrt(mean_squared_error(y_test_encoded, y_pred_prob_nn))\n",
    " \n",
    "print(\"\\nRésultats Réseau de Neurones:\")\n",
    "print(f\"Accuracy: {nn_accuracy:.4f}\")\n",
    "print(f\"AUC: {nn_auc:.4f}\")\n",
    "print(f\"RMSE: {nn_rmse:.4f}\")\n",
    " \n",
    "# Résumé final des performances\n",
    "print(\"\\n--- Résumé des performances ---\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric == \"Confusion Matrix\":\n",
    "            print(f\"  {metric}:\\n{value}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value:.4f}\" if value is not None else f\"  {metric}: N/A\")\n",
    "print(f\"Neural Network:\")\n",
    "print(f\"  Accuracy: {nn_accuracy:.4f}\")\n",
    "print(f\"  AUC: {nn_auc:.4f}\")\n",
    "print(f\"  RMSE: {nn_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
